{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "URL_BASE = 'https://www.argenprop.com'\n",
    "\n",
    "\n",
    "def obtener_enlaces_desde_inicio():\n",
    "    print(\"Obteniendo enlaces desde la página de inicio...\")\n",
    "    respuesta = requests.get(URL_BASE)\n",
    "    sopa = BeautifulSoup(respuesta.content, 'html.parser')\n",
    "\n",
    "    enlaces = []\n",
    "    for h3 in sopa.find_all('h3', class_='btn btn-text btn-block'):\n",
    "        etiqueta_a = h3.find('a')\n",
    "        if etiqueta_a:\n",
    "            href = etiqueta_a['href']\n",
    "            if any(palabra in href for palabra in [\"departamento-alquiler\", \"casa-alquiler\", \"inmuebles-alquiler\"]):\n",
    "                enlaces.append(href)\n",
    "\n",
    "    print(f\"Se obtuvieron {len(enlaces) - 3} enlaces desde la página de inicio.\")\n",
    "    return enlaces[3:]\n",
    "\n",
    "\n",
    "def obtener_todos_enlaces_de_pagina(pagina_url):\n",
    "    print(f\"Obteniendo enlaces desde la página: {pagina_url}...\")\n",
    "    page_number = 1\n",
    "    all_links = []\n",
    "    while True:\n",
    "        if page_number == 1:\n",
    "            url = pagina_url\n",
    "        else:\n",
    "            url = pagina_url + \"-pagina-\" + str(page_number)\n",
    "\n",
    "        respuesta = requests.get(url)\n",
    "\n",
    "        if respuesta.status_code != 200:\n",
    "            print(f\"Error al obtener la página {page_number}. Código de estado: {respuesta.status_code}\")\n",
    "            break\n",
    "\n",
    "        sopa = BeautifulSoup(respuesta.content, 'html.parser')\n",
    "        enlaces_pagina = [enlace['href'] for enlace in sopa.find_all('a', class_='card') if enlace.has_attr('href')]\n",
    "\n",
    "        if not enlaces_pagina:\n",
    "            break\n",
    "\n",
    "        all_links.extend(enlaces_pagina)\n",
    "        page_number += 1\n",
    "\n",
    "    print(f\"Se obtuvieron {len(all_links)} enlaces desde la página: {pagina_url}.\")\n",
    "    return all_links\n",
    "\n",
    "\n",
    "def extraer_datos_de_enlace(enlace):\n",
    "    print(f\"Extrayendo datos del enlace: {URL_BASE + enlace}...\")\n",
    "    respuesta = requests.get(URL_BASE + enlace)\n",
    "\n",
    "    if respuesta.status_code != 200:\n",
    "        print(f\"Error al acceder al enlace: {enlace}. Código de estado: {respuesta.status_code}\")\n",
    "        return None\n",
    "\n",
    "    soup = BeautifulSoup(respuesta.content, 'html.parser')\n",
    "\n",
    "    try:\n",
    "        location = soup.find('h3', class_='titlebar__address').text\n",
    "        price = soup.find('p', class_='titlebar__price').text.strip()\n",
    "        features_list_element = soup.find('ul', class_='property-main-features')\n",
    "        features_list = features_list_element.find_all('li') if features_list_element else []\n",
    "        features = {}\n",
    "        for feature in features_list:\n",
    "            key = feature.attrs['title']\n",
    "            value = feature.find('div', class_='desktop').p.text.strip()\n",
    "            features[key] = value\n",
    "        expenses_element = soup.find('p', class_='titlebar__expenses')\n",
    "        expenses = expenses_element.text.strip() if expenses_element else \"No se encontraron expensas\"\n",
    "\n",
    "        data = {\n",
    "            'URL': URL_BASE + enlace,\n",
    "            'Ubicación': location,\n",
    "            'Precio': price,\n",
    "            **features,\n",
    "            'Expensas': expenses\n",
    "        }\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        print(f\"Error al extraer datos del enlace: {enlace}. Error: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def main():\n",
    "    enlaces_inicio = obtener_enlaces_desde_inicio()\n",
    "    todos_enlaces = []\n",
    "\n",
    "    for enlace in enlaces_inicio:\n",
    "        todos_enlaces.extend(obtener_todos_enlaces_de_pagina(enlace))\n",
    "\n",
    "    datos_lista = []\n",
    "    for enlace in todos_enlaces:\n",
    "        data = extraer_datos_de_enlace(enlace)\n",
    "        if data:\n",
    "            datos_lista.append(data)\n",
    "\n",
    "    df = pd.DataFrame(datos_lista)\n",
    "    df.to_csv('propiedades_argenprop.csv', index=False)\n",
    "    print(\"¡Datos guardados exitosamente en 'propiedades_argenprop.csv'!\")\n",
    "\n",
    "    # Para visualizar el dataframe\n",
    "    print(df.head())\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
